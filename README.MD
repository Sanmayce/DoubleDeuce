To my knowledge, DoubleDeuce is the Fastest 128bit hash for hashing small keys (<1024 bytes).
It is used in my Zennish MicroDeduplicator Nakamichi 'DoubleDeuce' to hash 16+ bytes long matches down to 16 bytes.



The following code is the C code written by Kaze (sanmayce@sanmayce.com):
    
```

```
    
The following code is the ASM generated by https://godbolt.org/ x86-64 clang 20.1.0 with compile options: -O3 -maes -mavx2
    
```
SlowCopy128bit:
  vmovups xmm0, xmmword ptr [rdi]
  vmovups xmmword ptr [rsi], xmm0
  ret

.LCPI1_0:
  .quad 3285590903173248716
  .quad -2511163900754046922
.LCPI1_1:
  .quad 1162971903282775349
  .quad -3985312487901316173
.LCPI1_2:
  .quad 7113472399480571277
  .quad 7809847782465536322
.LCPI1_4:
  .byte 15
  .byte 14
  .byte 13
  .byte 12
  .byte 11
  .byte 10
  .byte 9
  .byte 8
  .byte 7
  .byte 6
  .byte 5
  .byte 4
  .byte 3
  .byte 2
  .byte 1
  .byte 0
.LCPI1_5:
  .byte 0
  .byte 8
  .byte 1
  .byte 9
  .byte 2
  .byte 10
  .byte 3
  .byte 11
  .byte 4
  .byte 12
  .byte 5
  .byte 13
  .byte 6
  .byte 14
  .byte 7
  .byte 15
.LCPI1_6:
  .byte 15
  .byte 7
  .byte 14
  .byte 6
  .byte 13
  .byte 5
  .byte 12
  .byte 4
  .byte 11
  .byte 3
  .byte 10
  .byte 2
  .byte 9
  .byte 1
  .byte 8
  .byte 0
DoubleDeuceAES_Gumbotron_YMM:
  cmp rsi, 64
  jae .LBB1_2
  vmovdqa xmm0, xmmword ptr [rip + .LCPI1_0]
  vmovdqa xmm2, xmmword ptr [rip + .LCPI1_1]
  vpxor xmm1, xmm1, xmm1
  vmovdqa xmm3, xmmword ptr [rip + .LCPI1_2]
  cmp rsi, 16
  jae .LBB1_6
  jmp .LBB1_10
.LBB1_2:
  mov rcx, rsi
  shr rcx, 6
  mov rax, rsi
  and rax, -64
  and esi, 63
  vmovdqa xmm3, xmmword ptr [rip + .LCPI1_2]
  vpxor xmm1, xmm1, xmm1
  vmovdqa xmm2, xmmword ptr [rip + .LCPI1_1]
  vmovdqa xmm0, xmmword ptr [rip + .LCPI1_0]
  vbroadcasti128 ymm4, xmmword ptr [rip + .LCPI1_4]
  mov rdx, rdi
.LBB1_3:
  vmovdqu ymm6, ymmword ptr [rdx]
  vmovdqu ymm7, ymmword ptr [rdx + 32]
  vpshufb ymm8, ymm7, ymm4
  vpshufb ymm5, ymm6, ymm4
  vpermq ymm9, ymm8, 78
  vpermq ymm10, ymm5, 78
  vpunpcklbw ymm11, ymm6, ymm7
  vpunpckhbw ymm12, ymm6, ymm7
  vaesenc xmm3, xmm3, xmm6
  vpunpcklbw ymm6, ymm9, ymm10
  vextracti128 xmm13, ymm8, 1
  vaesenc xmm0, xmm0, xmm13
  vaesenc xmm2, xmm2, xmm11
  vaesenc xmm1, xmm1, xmm6
  vpunpckhbw ymm9, ymm9, ymm10
  vaesenc xmm3, xmm3, xmmword ptr [rdx + 16]
  vaesenc xmm0, xmm0, xmm8
  vaesenc xmm2, xmm2, xmm12
  vaesenc xmm1, xmm1, xmm9
  vaesenc xmm3, xmm3, xmm7
  vextracti128 xmm7, ymm5, 1
  vaesenc xmm0, xmm0, xmm7
  vextracti128 xmm7, ymm11, 1
  vaesenc xmm2, xmm2, xmm7
  vextracti128 xmm6, ymm6, 1
  vaesenc xmm3, xmm3, xmmword ptr [rdx + 48]
  vaesenc xmm1, xmm1, xmm6
  vaesenc xmm0, xmm0, xmm5
  vextracti128 xmm5, ymm12, 1
  vaesenc xmm2, xmm2, xmm5
  add rdx, 64
  vextracti128 xmm5, ymm9, 1
  vaesenc xmm1, xmm1, xmm5
  vaesenc xmm3, xmm3, xmm0
  vaesenc xmm3, xmm3, xmm2
  vaesenc xmm3, xmm3, xmm1
  dec rcx
  jne .LBB1_3
  add rdi, rax
  cmp rsi, 16
  jb .LBB1_10
.LBB1_6:
  mov rcx, rsi
  shr rcx, 4
  mov eax, esi
  vmovdqu xmm4, xmmword ptr [rdi]
  vpshufb xmm5, xmm4, xmmword ptr [rip + .LCPI1_4]
  vpshufb xmm6, xmm4, xmmword ptr [rip + .LCPI1_5]
  vpshufb xmm7, xmm4, xmmword ptr [rip + .LCPI1_6]
  vaesenc xmm3, xmm3, xmm4
  vaesenc xmm0, xmm0, xmm5
  vaesenc xmm2, xmm2, xmm6
  vaesenc xmm1, xmm1, xmm7
  vaesenc xmm3, xmm3, xmm0
  vaesenc xmm3, xmm3, xmm2
  and eax, 48
  vaesenc xmm3, xmm3, xmm1
  cmp rcx, 1
  je .LBB1_9
  vmovdqu xmm4, xmmword ptr [rdi + 16]
  vpshufb xmm5, xmm4, xmmword ptr [rip + .LCPI1_4]
  vpshufb xmm6, xmm4, xmmword ptr [rip + .LCPI1_5]
  vpshufb xmm7, xmm4, xmmword ptr [rip + .LCPI1_6]
  vaesenc xmm3, xmm3, xmm4
  vaesenc xmm0, xmm0, xmm5
  vaesenc xmm2, xmm2, xmm6
  vaesenc xmm1, xmm1, xmm7
  vaesenc xmm3, xmm3, xmm0
  vaesenc xmm3, xmm3, xmm2
  vaesenc xmm3, xmm3, xmm1
  cmp rcx, 2
  je .LBB1_9
  vmovdqu xmm4, xmmword ptr [rdi + 32]
  vpshufb xmm5, xmm4, xmmword ptr [rip + .LCPI1_4]
  vpshufb xmm6, xmm4, xmmword ptr [rip + .LCPI1_5]
  vpshufb xmm7, xmm4, xmmword ptr [rip + .LCPI1_6]
  vaesenc xmm3, xmm3, xmm4
  vaesenc xmm0, xmm0, xmm5
  vaesenc xmm2, xmm2, xmm6
  vaesenc xmm1, xmm1, xmm7
  vaesenc xmm3, xmm3, xmm0
  vaesenc xmm3, xmm3, xmm2
  vaesenc xmm3, xmm3, xmm1
.LBB1_9:
  add rdi, rax
  and esi, 15
.LBB1_10:
  test rsi, rsi
  je .LBB1_12
  shl rsi, 4
  lea rax, [rip + VectorsNeedNonVAriable1]
  vmovdqa xmm4, xmmword ptr [rsi + rax]
  vpand xmm4, xmm4, xmmword ptr [rdi]
  lea rax, [rip + VectorsNeedNonVAriable2]
  vpand xmm5, xmm0, xmmword ptr [rsi + rax]
  vpor xmm4, xmm5, xmm4
  vpshufb xmm5, xmm4, xmmword ptr [rip + .LCPI1_4]
  vpshufb xmm6, xmm4, xmmword ptr [rip + .LCPI1_5]
  vaesenc xmm2, xmm2, xmm6
  vpshufb xmm6, xmm4, xmmword ptr [rip + .LCPI1_6]
  vaesenc xmm1, xmm1, xmm6
  vaesenc xmm3, xmm3, xmm4
  vaesenc xmm0, xmm0, xmm5
  vaesenc xmm0, xmm3, xmm0
  vaesenc xmm0, xmm0, xmm2
  vaesenc xmm3, xmm0, xmm1
.LBB1_12:
  vmovdqa xmmword ptr [rip + DDAES], xmm3
  vzeroupper
  ret

DDAES:
  .zero 16

VectorsNeedNonVAriable1:
  .asciz "\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377"

VectorsNeedNonVAriable2:
  .ascii "\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\377\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\377\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\377\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\377\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\377\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\377\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\377\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\377\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\377\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\377\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\377\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377\377\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\377"
```
